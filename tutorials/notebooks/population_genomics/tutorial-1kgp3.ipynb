{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TileDB VCF Example for 1000 Genomes Project\n",
    "\n",
    "This tutorial provides a walkthrough of [TileDB-VCF](https://github.com/TileDB-Inc/TileDB-VCF.git) using Phase 3 data produced by the [1000 Genomes Project](https://www.internationalgenome.org/). The goal is to highlight storing these samples in TileDB-VCF along with querying in python and exporting back to VCF and TSV.\n",
    "\n",
    "Please see the [official documentation](https://docs.tiledb.com/solutions/integrations/population-genomics) for more comprehensive usage details, API references, as well as instructions for installing TileDB-VCF.\n",
    "\n",
    "## Preprocessing the Raw 1KG pVCF File\n",
    "\n",
    "*Note: We're only working with data from chromosome 1 for the purposes of this tutorial. However, the process of working with whole genome data is the same.*\n",
    "\n",
    "The original VCF file was downloaded from the AWS Open Data Registry: <a href=\"https://registry.opendata.aws/1000-genomes\" class=\"uri\">https://registry.opendata.aws/1000-genomes</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://1000genomes/release/20130502/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz to data/1000genomes/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 sync \\\n",
    "    --exclude \"*\" --include \"ALL.chr1.*\" \\\n",
    "    --no-sign-request\\\n",
    "    s3://1000genomes/release/20130502/ \\\n",
    "    data/1000genomes/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the more modern high-coverage version of the thousand genomes (1KG) data, which provides the raw single-sample gVCF files TileDB-VCF was designed for, the low-coverage Phase 3 data provides only cohort-level project VCF (pVCF) files for each contig. Therefore, we must first split the pVCF file back into single-sample VCF files prior to ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bcftools +split \\\n",
    "    -Ob \\\n",
    "    -o data/split-bcfs \\\n",
    "    data/1000genomes/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we’ll filter the split VCF files to include only records with a non-reference allele and remove `INFO` attributes that are either static (e.g., `NS`) or cohort-specific and recoverable (e.g., `AF`). We’ll save the final pre-processed files in *BCF* format, which is a binary representation of the VCF format. The resulting filtered BCF files are a close approximation of the raw single sample VCF files typically stored for large population genomics projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm_tags=INFO/AF,INFO/NS,INFO/EAS_AF,INFO/AMR_AF,INFO/AFR_AF,INFO/EUR_AF,INFO/SAS_AF\n",
    "\n",
    "!ls data/split-bcfs/*.bcf | parallel -j16 \\\n",
    "    \"bcftools view --min-ac 1 -Ou -s {/.} {} | bcftools annotate -Ob --remove $rm_tags -o data/filtered-bcfs/{/}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new BCF files must also be indexed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data/filtered-bcfs/*.bcf | parallel -j16 \"bcftools index {}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we’ll store these pre-processed files on S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive data/filtered-bcfs/ s3://genomic-datasets/notebooks/1kgp3/filtered-bcfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Data in TileDB VCF\n",
    "\n",
    "Next we will create a TileDB VCF dataset and ingest the VCF data.\n",
    "\n",
    "The following was run on a `m5.4xlarge` system with a 300GB EBS volume to handle the large number of VCF files. Note that TileDB is *highly-tunable* and while the defaults were chosen to provide a good balance between ingestion speed, read performance, and dataset size, they can be be tweaked to better suit a specific use case.\n",
    "\n",
    "### Create the dataset\n",
    "\n",
    "You can create a TileDB VCF dataset anywhere that TileDB supports, this could be a local filesystem, s3, azure, gcs, hdfs, and more. For this example we’ll create it on s3, the most common use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the path below with your own s3 bucket\n",
    "!tiledbvcf create -u s3://genomic-datasets/notebooks/1kgp3/1kg-array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing the data\n",
    "\n",
    "To store the VCF data in TileDB VCF you simply need a list of the VCF/BCF file locations (e.g., local file paths, S3 URIs, *etc*).\n",
    "\n",
    "Run the following command to ingest the pre-processed BCF files into the TileDB-VCF dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tiledbvcf store -u s3://genomic-datasets/notebooks/1kgp3/1kg-array \\\n",
    "    --threads 16 \\\n",
    "    --sample-batch-size 20 \\\n",
    "    --verbose \\\n",
    "    --stats \\\n",
    "    data/filtered-bcfs/*.bcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running with `--verbose` you will get a summary printed at the end: `Done. Ingested 857,389,804 records (+ 4,076,523 anchors) from 2,504 samples in 2,133.21 seconds.`\n",
    "\n",
    "This indicates we’ve ingested over 857 million genomic positions into the TileDB-VCF dataset. With an `m5.4xlarge` instance costing \\$0.768 an hour, and this ingestion took just under 40 minutes, the cost of ingestion was \\$0.50 USD, so ingesting all data from the 1KG Phase results would cost roughly $7.00.\n",
    "\n",
    "The final array is 6Gb, or just under half the size of the individual compressed BCF files.\n",
    "\n",
    "Following ingestion, it may help performance to consolidate the metadata fragments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tiledbvcf utils consolidate fragment_meta -u s3://genomic-datasets/notebooks/1kgp3/1kg-array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading, Analysis and Exporting\n",
    "\n",
    "In this section we will walk through accessing the TileDB-VCF dataset with python and also exporting back to VCF and TSV.\n",
    "\n",
    "### Python API\n",
    "\n",
    "TileDB-VCF offers several APIs, for this section we will focus on the Python API. First we load the module and setup a few variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiledbvcf\n",
    "\n",
    "uri = \"s3://genomic-datasets/notebooks/1kgp3/1kg-array\"\n",
    "bedfile = \"s3://genomic-datasets/notebooks/1kgp3/hg37_chr1_covidHgiGwasR4PvalC2_plog3.bed.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading into Pandas Dataframe\n",
    "\n",
    "Pandas is one of the most popular data science tools in python. TileDB VCF’s python API produces results directly into a pandas dataframe. This makes its easy to analyze the data and leverage any of pandas’ builtin algorithms.\n",
    "\n",
    "Let’s run a typical query on the 1kg TileDB-VCF dataset we created above. We’ll retrieve all variants that overlap the gene *MTOR* on chr 1 for sample HG00096, along with a few attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_name</th>\n",
       "      <th>contig</th>\n",
       "      <th>pos_start</th>\n",
       "      <th>pos_end</th>\n",
       "      <th>alleles</th>\n",
       "      <th>fmt_GT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43337897</td>\n",
       "      <td>43337897</td>\n",
       "      <td>[A, G]</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43339092</td>\n",
       "      <td>43339092</td>\n",
       "      <td>[C, T]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43339203</td>\n",
       "      <td>43339203</td>\n",
       "      <td>[G, A]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43340776</td>\n",
       "      <td>43340776</td>\n",
       "      <td>[T, C]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43340779</td>\n",
       "      <td>43340779</td>\n",
       "      <td>[A, G]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43341662</td>\n",
       "      <td>43341662</td>\n",
       "      <td>[A, G]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43342021</td>\n",
       "      <td>43342021</td>\n",
       "      <td>[G, A]</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43343390</td>\n",
       "      <td>43343390</td>\n",
       "      <td>[A, G]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43344193</td>\n",
       "      <td>43344193</td>\n",
       "      <td>[G, A]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43344632</td>\n",
       "      <td>43344632</td>\n",
       "      <td>[T, A]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43346551</td>\n",
       "      <td>43346551</td>\n",
       "      <td>[A, G]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43347556</td>\n",
       "      <td>43347556</td>\n",
       "      <td>[C, G]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43347925</td>\n",
       "      <td>43347925</td>\n",
       "      <td>[C, T]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43348081</td>\n",
       "      <td>43348081</td>\n",
       "      <td>[G, T]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43348184</td>\n",
       "      <td>43348184</td>\n",
       "      <td>[A, T]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43350603</td>\n",
       "      <td>43350603</td>\n",
       "      <td>[C, T]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43351984</td>\n",
       "      <td>43351984</td>\n",
       "      <td>[A, G]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43352685</td>\n",
       "      <td>43352685</td>\n",
       "      <td>[A, G]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_name contig  pos_start   pos_end alleles  fmt_GT\n",
       "0      HG00096      1   43337897  43337897  [A, G]  [1, 0]\n",
       "1      HG00096      1   43339092  43339092  [C, T]  [1, 1]\n",
       "2      HG00096      1   43339203  43339203  [G, A]  [1, 1]\n",
       "3      HG00096      1   43340776  43340776  [T, C]  [1, 1]\n",
       "4      HG00096      1   43340779  43340779  [A, G]  [1, 1]\n",
       "5      HG00096      1   43341662  43341662  [A, G]  [1, 1]\n",
       "6      HG00096      1   43342021  43342021  [G, A]  [1, 0]\n",
       "7      HG00096      1   43343390  43343390  [A, G]  [1, 1]\n",
       "8      HG00096      1   43344193  43344193  [G, A]  [1, 1]\n",
       "9      HG00096      1   43344632  43344632  [T, A]  [1, 1]\n",
       "10     HG00096      1   43346551  43346551  [A, G]  [1, 1]\n",
       "11     HG00096      1   43347556  43347556  [C, G]  [1, 1]\n",
       "12     HG00096      1   43347925  43347925  [C, T]  [1, 1]\n",
       "13     HG00096      1   43348081  43348081  [G, T]  [1, 1]\n",
       "14     HG00096      1   43348184  43348184  [A, T]  [1, 1]\n",
       "15     HG00096      1   43350603  43350603  [C, T]  [1, 1]\n",
       "16     HG00096      1   43351984  43351984  [A, G]  [1, 1]\n",
       "17     HG00096      1   43352685  43352685  [A, G]  [1, 1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = tiledbvcf.ReadConfig(memory_budget_mb=8192)\n",
    "\n",
    "ds = tiledbvcf.Dataset(uri, stats = True, cfg = cfg)\n",
    "\n",
    "ds.read(\n",
    "    attrs = [\"sample_name\", \"contig\", \"pos_start\", \"pos_end\", \"alleles\", \"fmt_GT\"], \n",
    "    regions = [\"1:43337848-43352772\"],\n",
    "    samples = [\"HG00096\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the `Dataset` object was created with `stats = True` you can print out a variety of useful information about the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== READ ====\n",
      "\n",
      "- Number of read queries: 3\n",
      "- Number of attempts until results are found: 3\n",
      "\n",
      "- Number of attributes read: 5\n",
      "  * Number of fixed-sized attributes read: 2\n",
      "  * Number of var-sized attributes read: 3\n",
      "- Number of dimensions read: 5\n",
      "  * Number of fixed-sized dimensions read: 1\n",
      "  * Number of var-sized dimensions read: 4\n",
      "\n",
      "- Number of logical tiles overlapping the query: 128\n",
      "- Number of physical tiles read: 2176\n",
      "  * Number of physical fixed-sized tiles read: 384\n",
      "  * Number of physical var-sized tiles read: 1792\n",
      "- Number of cells read: 12524\n",
      "- Number of result cells: 2524\n",
      "- Percentage of useful cells read: 20.1533%\n",
      "\n",
      "- Number of bytes read: 316500 bytes (0.000294764 GB) \n",
      "- Number of read operations: 2728\n",
      "- Number of bytes unfiltered: 1404653 bytes (0.00130819 GB) \n",
      "- Unfiltering inflation factor: 4.43808x\n",
      "\n",
      "- Time to compute estimated result size: 0.226271 secs\n",
      "  * Time to compute tile overlap: 0.296438 secs\n",
      "    > Time to compute relevant fragments: 0.000684499 secs\n",
      "    > Time to load relevant fragment R-trees: 0.292355 secs\n",
      "    > Time to compute relevant fragment tile overlap: 0.00334145 secs\n",
      "\n",
      "- Time to open array: 1.47771 secs\n",
      "  * Time to load array schema: 0.0642025 secs\n",
      "  * Time to load consolidated fragment metadata: 0.0381051 secs\n",
      "  * Time to load fragment metadata: 1.25327 secs\n",
      "\n",
      "- Total metadata read: 359434 bytes (0.000334749 GB) \n",
      "  * Array schema: 766 bytes (7.13393e-07 GB) \n",
      "  * Consolidated fragment metadata: 130292 bytes (0.000121344 GB) \n",
      "  * Fragment metadata: 96012 bytes (8.94181e-05 GB) \n",
      "  * R-tree: 46268 bytes (4.30904e-05 GB) \n",
      "  * Fixed-sized tile offsets: 38432 bytes (3.57926e-05 GB) \n",
      "  * Var-sized tile offsets: 22832 bytes (2.1264e-05 GB) \n",
      "  * Var-sized tile sizes: 24832 bytes (2.31266e-05 GB) \n",
      "\n",
      "- Time to load array metadata: 0.0971574 secs\n",
      "  * Array metadata size: 102 bytes (9.49949e-08 GB) \n",
      "\n",
      "- Time to initialize the read state: 0.0727732 secs\n",
      "\n",
      "- Read time: 0.527277 secs\n",
      "  * Time to compute next partition: 0.000906719 secs\n",
      "  * Time to compute result coordinates: 0.389981 secs\n",
      "    > Time to compute sparse result tiles: 0.0001632 secs\n",
      "    > Time to read coordinate tiles: 0.378157 secs\n",
      "    > Time to unfilter coordinate tiles: 0.00425095 secs\n",
      "    > Time to compute range result coordinates: 0.00219325 secs\n",
      "  * Time to compute sparse result cell slabs: 1.5181e-05 secs\n",
      "  * Time to copy result attribute values: 0.135654 secs\n",
      "    > Time to read attribute tiles: 0.133284 secs\n",
      "    > Time to unfilter attribute tiles: 0.00107886 secs\n",
      "    > Time to copy fixed-sized attribute values: 0.000267195 secs\n",
      "    > Time to copy var-sized attribute values: 0.000320791 secs\n",
      "  * Time to copy result coordinates: 0.000672115 secs\n",
      "    > Time to copy fixed-sized coordinates: 0.000131452 secs\n",
      "    > Time to copy var-sized coordinates: 0.000436485 secs\n",
      "\n",
      "- Total read query time (array open + init state + read): 0.60005 secs\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ds.tiledb_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above query took under a second for a single sample. Running this same query across all 2,504 samples took only 8.2 secs.\n",
    "\n",
    "For production-sized queries that encompass large portions of the genome it's more convenient to provide bed files with the query regions. Here, we'll use a bed file on s3 that contains 1,040 regions on chr1 that show at least a moderate association with with SARS-CoV-2 infection susceptibility (data obtained from the [COVID-19 Host Genetics Initiative](https://www.covid19hg.org/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_name</th>\n",
       "      <th>contig</th>\n",
       "      <th>pos_start</th>\n",
       "      <th>pos_end</th>\n",
       "      <th>alleles</th>\n",
       "      <th>info_DP</th>\n",
       "      <th>fmt_GT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HG00097</td>\n",
       "      <td>1</td>\n",
       "      <td>2265099</td>\n",
       "      <td>2265099</td>\n",
       "      <td>[C, T]</td>\n",
       "      <td>[16479]</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HG00099</td>\n",
       "      <td>1</td>\n",
       "      <td>2265099</td>\n",
       "      <td>2265099</td>\n",
       "      <td>[C, T]</td>\n",
       "      <td>[16479]</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HG00107</td>\n",
       "      <td>1</td>\n",
       "      <td>2265099</td>\n",
       "      <td>2265099</td>\n",
       "      <td>[C, T]</td>\n",
       "      <td>[16479]</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>2337537</td>\n",
       "      <td>2337537</td>\n",
       "      <td>[C, G]</td>\n",
       "      <td>[11721]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HG00097</td>\n",
       "      <td>1</td>\n",
       "      <td>2337537</td>\n",
       "      <td>2337537</td>\n",
       "      <td>[C, G]</td>\n",
       "      <td>[11721]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3470</th>\n",
       "      <td>HG00102</td>\n",
       "      <td>1</td>\n",
       "      <td>247999680</td>\n",
       "      <td>247999680</td>\n",
       "      <td>[G, A]</td>\n",
       "      <td>[19889]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3471</th>\n",
       "      <td>HG00103</td>\n",
       "      <td>1</td>\n",
       "      <td>247999680</td>\n",
       "      <td>247999680</td>\n",
       "      <td>[G, A]</td>\n",
       "      <td>[19889]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3472</th>\n",
       "      <td>HG00105</td>\n",
       "      <td>1</td>\n",
       "      <td>247999680</td>\n",
       "      <td>247999680</td>\n",
       "      <td>[G, A]</td>\n",
       "      <td>[19889]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3473</th>\n",
       "      <td>HG00106</td>\n",
       "      <td>1</td>\n",
       "      <td>247999680</td>\n",
       "      <td>247999680</td>\n",
       "      <td>[G, A]</td>\n",
       "      <td>[19889]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3474</th>\n",
       "      <td>HG00107</td>\n",
       "      <td>1</td>\n",
       "      <td>247999680</td>\n",
       "      <td>247999680</td>\n",
       "      <td>[G, A]</td>\n",
       "      <td>[19889]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3475 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sample_name contig  pos_start    pos_end alleles  info_DP  fmt_GT\n",
       "0        HG00097      1    2265099    2265099  [C, T]  [16479]  [0, 1]\n",
       "1        HG00099      1    2265099    2265099  [C, T]  [16479]  [0, 1]\n",
       "2        HG00107      1    2265099    2265099  [C, T]  [16479]  [1, 0]\n",
       "3        HG00096      1    2337537    2337537  [C, G]  [11721]  [1, 1]\n",
       "4        HG00097      1    2337537    2337537  [C, G]  [11721]  [1, 1]\n",
       "...          ...    ...        ...        ...     ...      ...     ...\n",
       "3470     HG00102      1  247999680  247999680  [G, A]  [19889]  [1, 1]\n",
       "3471     HG00103      1  247999680  247999680  [G, A]  [19889]  [1, 1]\n",
       "3472     HG00105      1  247999680  247999680  [G, A]  [19889]  [1, 1]\n",
       "3473     HG00106      1  247999680  247999680  [G, A]  [19889]  [1, 1]\n",
       "3474     HG00107      1  247999680  247999680  [G, A]  [19889]  [1, 1]\n",
       "\n",
       "[3475 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = tiledbvcf.Dataset(uri, stats = True, cfg = cfg)\n",
    "\n",
    "df = ds.read(\n",
    "    attrs = [\"sample_name\", \"contig\", \"pos_start\", \"pos_end\", \"alleles\", \"info_DP\", \"fmt_GT\"], \n",
    "    samples = ds.samples()[:10],\n",
    "    bed_file = bedfile\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This query completed in 1.9 secs for 10 samples.\n",
    "\n",
    "#### Filter Example\n",
    "\n",
    "Using the pandas dataframe returned from TileDB-VCF we can apply additional filters. For instance if we wanted to filter the above result on read depth (`info_DP`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.info_DP.apply(lambda x: x[0] > 5000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python API supports a variety of advanced uses, batching, partitioning, dask and more. We are happy to follow-up with additional details beyond these initial examples.\n",
    "\n",
    "### CLI Exporting\n",
    "\n",
    "In addition to the python API it is also possible to export the dataset back into VCF format. This can be helpful in interoperating with legacy tools.\n",
    "\n",
    "#### Exporting to VCF\n",
    "\n",
    "When exporting to VCF you can specify any number of samples, and each will be exported to its own file in vcf, compressed vcf or bcf format depdning on what you set for `--output-format`.\n",
    "\n",
    "For example to export the entire sample for `HG00096`, `HG00097`, and `HG00099` you can run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tiledbvcf export --uri s3://genomic-datasets/notebooks/1kgp3/1kg-array \\\n",
    "    --output-format v \\\n",
    "    --sample-names HG00096,HG00097,HG00099 \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces 3 files: `HG00096.vcf`, `HG00097.vcf`, and `HG00099.vcf`.\n",
    "\n",
    "##### Filtering Exports\n",
    "\n",
    "You can also combine the use of regions (bed file or list of regions passed to cli) to filter the export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tiledbvcf export --uri s3://genomic-datasets/notebooks/1kgp3/1kg3-array \\\n",
    "    --output-format v \\\n",
    "    --sample-names HG00096,HG00097,HG00099 \\\n",
    "    --regions-file s3://genomic-datasets/notebooks/1kgp3/hg37_chr1_covidHgiGwasR4PvalC2_plog3.bed.gz \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will also produce the 3 VCF files, like the previous export. However, these files are filtered for the same SARS-CoV-2 associated genomic regions specified in the bed file.\n",
    "\n",
    "\n",
    "#### Exporting to TSV\n",
    "\n",
    "For even more generic usecases you can export data to tab seperate files with the `--output-format t` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tiledbvcf export --uri s3://genomic-datasets/notebooks/1kgp3/1kg-array \\\n",
    "    --output-format t \\\n",
    "    --tsv-fields CHR,POS,I:END,REF,ALT,S:GT,Q:POS,Q:END \\\n",
    "    --sample-names HG00096,HG00097,HG00099 \\\n",
    "    --regions-file s3://genomic-datasets/notebooks/1kgp3/hg37_chr1_covidHgiGwasR4PvalC2_plog3.bed.gz \\\n",
    "    --verbose --output-path sars-cov-2-associated-regions.tsv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
